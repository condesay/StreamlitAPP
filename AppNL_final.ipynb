{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZMRjNwqU4Xe"
      },
      "source": [
        "\n",
        "<br>\n",
        "Description<br>\n",
        "Application NLP traitant:<br>\n",
        "+ Tokenization & Lemmatization utilisant Spacy<br>\n",
        "+ Named Entity Recognition(NER) utilisant SpaCy<br>\n",
        "+ Sentiment Analysis utilisant TextBlob<br>\n",
        "+ Document/Text Resu,é utilisant Gensim/Sumy<br>\n",
        "Construit avec  Streamlit Framework, un très bon framework pour les projets ML and NLP avec peu de codes et facile à comprendre.<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAi5neV3KWX6"
      },
      "source": [
        "Probleme à resoudre:\n",
        "Gensim summarization et sentiment analysis donné directement positif ou negatif aulieu de polarité et subjectivité"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX9s-lbpU4Xh",
        "outputId": "7301b9d8-3743-49e1-ffd6-4118be3a8a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.17.0-py2.py3-none-any.whl (9.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semver\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.8/dist-packages (from streamlit) (1.3.5)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from streamlit) (4.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from streamlit) (2.8.2)\n",
            "Collecting blinker>=1.0.0\n",
            "  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
            "Collecting validators>=0.2\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.8/dist-packages (from streamlit) (6.0.4)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from streamlit) (1.21.6)\n",
            "Collecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.8/dist-packages (from streamlit) (6.0.0)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.8/dist-packages (from streamlit) (5.2.1)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.8/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.8/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: packaging>=14.1 in /usr/local/lib/python3.8/dist-packages (from streamlit) (21.3)\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.8/dist-packages (from streamlit) (3.19.6)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from streamlit) (4.2.0)\n",
            "Collecting rich>=10.11.0\n",
            "  Downloading rich-13.3.1-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.0/239.0 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchdog\n",
            "  Downloading watchdog-2.2.1-py3-none-manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.0/79.0 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.4 in /usr/local/lib/python3.8/dist-packages (from streamlit) (2.25.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit) (0.12.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit) (4.3.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=1.4->streamlit) (3.11.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=14.1->streamlit) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.21.0->streamlit) (2022.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil->streamlit) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4->streamlit) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4->streamlit) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4->streamlit) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.4->streamlit) (4.0.0)\n",
            "Collecting pygments<3.0.0,>=2.14.0\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown-it-py<3.0.0,>=2.1.0\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from validators>=0.2->streamlit) (4.4.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (22.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (0.19.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit) (5.10.2)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Building wheels for collected packages: validators\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19581 sha256=fb6a9757410401cab601c775d465a450f8791260bc9e234ada7132964bab1f9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/09/72/3eb74d236bb48bd0f3c6c3c83e4e0c5bbfcbcad7c6c3539db8\n",
            "Successfully built validators\n",
            "Installing collected packages: watchdog, validators, smmap, semver, pympler, pygments, mdurl, blinker, pydeck, markdown-it-py, gitdb, rich, gitpython, streamlit\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blinker-1.5 gitdb-4.0.10 gitpython-3.1.30 markdown-it-py-2.1.0 mdurl-0.1.2 pydeck-0.8.0 pygments-2.14.0 pympler-1.0.1 rich-13.3.1 semver-2.13.0 smmap-5.0.0 streamlit-1.17.0 validators-0.20.0 watchdog-2.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit\n",
        "import os\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKSG6na5_-k1",
        "outputId": "7cc03b72-48a8-4f12-9c3e-7531921421dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E: Unable to locate package streamlit\n",
            "/bin/bash: import: command not found\n"
          ]
        }
      ],
      "source": [
        "!apt-get -qq install streamlit\n",
        "!import streamlit as st"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsXmucjuU4Xi"
      },
      "source": [
        "NLP Pkgs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07teE8d0U4Xi"
      },
      "source": [
        "Sumy Summary Pkg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY8t2p5OU4Xj",
        "outputId": "144c35b4-1417-4e8b-e67e-4a0a502be884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt<0.7,>=0.6.1\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.8/dist-packages (from sumy) (2.25.1)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from sumy) (3.7)\n",
            "Collecting pycountry>=18.2.23\n",
            "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.8/dist-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.8/dist-packages (from breadability>=0.1.20->sumy) (4.9.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.0.2->sumy) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.0.2->sumy) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.0.2->sumy) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk>=3.0.2->sumy) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from pycountry>=18.2.23->sumy) (57.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.7.0->sumy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.7.0->sumy) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.7.0->sumy) (2.10)\n",
            "Building wheels for collected packages: breadability, docopt, pycountry\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21714 sha256=d89dbf2046a8d616579cc6ea46d96db8a1c193d026504188240444c35094842e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/0d/0c/2062d8c1758b4b1a2e42b4a63e6660d9ec2ba9463cfee9eeab\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=ce02f7512ee6b4e0ed3a238b9d09139035f2d93ee4db6333db9ab8de9243a731\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "  Building wheel for pycountry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681845 sha256=26605633d044aea9f340e64233a96851e6cd7b92e6042706766a395f65309066\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/aa/0f/c224e473b464387170b83ca7c66947b4a7e33e8d903a679748\n",
            "Successfully built breadability docopt pycountry\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-22.3.5 sumy-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sumy\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwpnnkM_C-5Z",
        "outputId": "a725afd0-db2c-4d0b-b8cc-7b53280344bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBlIk76CU4Xj"
      },
      "source": [
        "Function for Sumy Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV7vEk0bU4Xj",
        "outputId": "69b1abf3-4954-4bb5-d8d2-0877bd08d894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile app.py\n",
        "# Core Pkgs\n",
        "import streamlit as st\n",
        "import os\n",
        "import re\n",
        "\n",
        "# NLP Pkgs\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "from gensim.summarization.summarizer import summarize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sumy Summary Pkg\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "# function gensim summarization\n",
        "def summarize(doc):\n",
        "   doc = re.sub(r'\\n|\\r', ' ', doc)\n",
        "   doc = re.sub(r' +', ' ', doc)\n",
        "   doc = doc.strip()\n",
        "   result_gen= \" \".join(doc.split()[:int(len(doc.split())/2)])\n",
        "   return result_gen\n",
        "# Function for Sumy Summarization\n",
        "def sumy_summarizer(docx):\n",
        "\tparser = PlaintextParser.from_string(docx,Tokenizer(\"english\"))\n",
        "\tlex_summarizer = LexRankSummarizer()\n",
        "\tsummary = lex_summarizer(parser.document,3)\n",
        "\tsummary_list = [str(sentence) for sentence in summary]\n",
        "\tresult = ' '.join(summary_list)\n",
        "\treturn result\n",
        "\n",
        "# Function to Analyse Tokens and Lemma\n",
        "@st.cache\n",
        "def text_analyzer(my_text):\n",
        "\tnlp = spacy.load('en_core_web_sm')\n",
        "\tdocx = nlp(my_text)\n",
        "\t# tokens = [ token.text for token in docx]\n",
        "\tallData = [('\"Token\":{},\\n\"Lemma\":{}'.format(token.text,token.lemma_))for token in docx ]\n",
        "\treturn allData\n",
        "\n",
        "# Function For Extracting Entities\n",
        "@st.cache\n",
        "def entity_analyzer(my_text):\n",
        "\tnlp = spacy.load('en_core_web_sm')\n",
        "\tdocx = nlp(my_text)\n",
        "\ttokens = [ token.text for token in docx]\n",
        "\tentities = [(entity.text,entity.label_)for entity in docx.ents]\n",
        "\tallData = ['\"Token\":{},\\n\"Entities\":{}'.format(tokens,entities)]\n",
        "\treturn allData\n",
        "\n",
        "# Enlever les caractères spéciaux , ponctuations\n",
        "def remove_special_characters(Description):\n",
        "    # define the pattern to keep\n",
        "    pat = r'[^a-zA-z0-9.,/:;\\\"\\'\\s]' \n",
        "    return re.sub(pat, '', Description)\n",
        "\n",
        "def main():\n",
        "\t\"\"\" NLP Application sur Streamlit \"\"\"\n",
        "\n",
        "\t# Title\n",
        "\tst.title(\" NLP Application\")\n",
        "\tst.subheader(\"Natural Language Processing Application\")\n",
        "\tst.markdown(\"\"\"\n",
        "    \t#### Description\n",
        "    \t+ C'est une application de Natural Language Processing(NLP) Basée sur du traitement automatique du language Naturel, entre autres nous avons: \n",
        "    \tTokenization , Lemmatization, Reconnaissance d'entités de nom (NER), Analyse de Sentiments ,  Résumé du texte...! \n",
        "    \t\"\"\")\n",
        "  \n",
        "\t# Summarization\n",
        "\tif st.checkbox(\"Trouvez le Résumé de votre texte\"):\n",
        "\t\tst.subheader(\"Résumez votre Texte\")\n",
        "\n",
        "\t\tmessage = st.text_area(\"Entrez le Texte à résumer\",\"Tapez Ici...\")\n",
        "\t\tsummary_options = st.selectbox(\"choisir méthode\",['sumy','gensim'])\n",
        "\t\tif st.button(\"Summarize\"):\n",
        "\t\t\tif summary_options == 'sumy':\n",
        "\t\t\t\tst.text(\"Utilisant la méthode Sumy ..\")\n",
        "\t\t\t\tsummary_result = sumy_summarizer(message)\n",
        "\t\t\telif summary_options == 'gensim':\n",
        "\t\t\t\tst.text(\"Utilisant la méthode Gensim  ..\")\n",
        "\t\t\t\tsummary_result = summarize(message)\n",
        "\t\t\telse:\n",
        "\t\t\t\tst.warning(\"Using Default Summarizer\")\n",
        "\t\t\t\tst.text(\"Utilisant la méthode Gensim  ..\")\n",
        "\t\t\t\tsummary_result = summarize(message)\n",
        "\t\t\tst.success(summary_result)\n",
        "  \n",
        "\t# Sentiment Analysis\n",
        "\tif st.checkbox(\"Trouvez le Sentiment de votre  texte\"):\n",
        "\t\tst.subheader(\"Identification du Sentiment dans votre Texte\")\n",
        "\n",
        "\t\tmessage = st.text_area(\"Entrez le Texte à identifier\",\"Tapez Ici...\")\n",
        "\t\tif st.button(\"Analyse\"):\n",
        "\t\t\tblob = TextBlob(message)\n",
        "\t\t\tresult_sentiment = blob.sentiment\n",
        "\t\t\tst.success(result_sentiment)\n",
        "\n",
        "\t# Entity Extraction\n",
        "\tif st.checkbox(\"Trouvez les  Entités de noms dans votre texte\"):\n",
        "\t\tst.subheader(\"Identification des Entités dans votre texte\")\n",
        "\n",
        "\t\tmessage = st.text_area(\"Entrez le Texte pour extraire NER\",\"Tapez Ici...\")\n",
        "\t\tif st.button(\"Extraction\"):\n",
        "\t\t\tentity_result = entity_analyzer(message)\n",
        "\t\t\tst.json(entity_result)\n",
        "\n",
        "\t# Tokenization \n",
        "\tif st.checkbox(\"Trouvez les Tokens et les Lemmas du texte\"):\n",
        "\t\tst.subheader(\"Tokenisez votre Texte\")\n",
        "\n",
        "\t\tmessage = st.text_area(\"Entrez le Texte à Tokeniser\",\"Tapez Ici...\")\n",
        "\t\tif st.button(\"Tokenise\"):\n",
        "\t\t\tnlp_result = text_analyzer(message)\n",
        "\t\t\tst.json(nlp_result)\n",
        "\t\t\t\n",
        " \t# Translate \n",
        "\tif st.checkbox(\"Trouvez la Traduction du  texte en Anglais\"):\n",
        "\t\tst.subheader(\"Traduisez votre Texte\")\n",
        "\n",
        "\t\tmessage = st.text_area(\"Entrez le Texte à traduire\",\"Tapez Ici...\")\n",
        "\t\tif st.button(\"Traduction\"):\n",
        "\t\t  \n",
        "\t\t\ttraduction = TextBlob(message).translate(from_lang=\"fr\",to=\"en\")\n",
        "\t\t\tst.success(traduction)\n",
        "\n",
        "\n",
        "  # Remove \n",
        "\tif st.checkbox(\"Effacez les caractère spéciaux du texte\"):\n",
        "\t\tst.subheader(\"Effacez les caractères spéciaux\")\n",
        "\n",
        "\t\tmessage = st.text_area(\"Entrez le Texte à nettoyer\",\"Tapez Ici...\")\n",
        "\t\tif st.button(\"Efface\"):\n",
        "\t\t  \n",
        "\t\t\tResultRemove = remove_special_characters(message)\n",
        "\t\t\tst.success(ResultRemove)    \n",
        "\n",
        "\tst.sidebar.subheader(\"Information sur  de l'Application de Bases de connaissances\")\n",
        "\tst.sidebar.text(\"BDC (Bases De Connaissances) Application.\")\n",
        "\tst.sidebar.info(\"Cette Application permet de trouver le sentiment score, les tokens et les lemmas dans une phrase ou texte, les entités de noms, suppressions des caractères sspéciaux et Resumé  du texte.!\")\n",
        "\t\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tmain()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyXBdhTcWaEA",
        "outputId": "24129791-6002-46d9-d6e6-5f5d3d6ccfe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 1.234s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[92m0\u001b[0m vulnerabilities\n",
            "\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KilNSegcRO9z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxJVaWNLU4Xk"
      },
      "source": [
        "Function to Analyse Tokens and Lem "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q3AiO10PAwv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "IqbfCNDsWPDQ"
      },
      "outputs": [],
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXdNHF7WWQKn",
        "outputId": "44d15d91-2ce9-4300-ee42-3ca1374e2e9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.873s\n",
            "your url is: https://late-hotels-rush-35-227-2-132.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}